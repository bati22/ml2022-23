{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETFpFlCmeuSB"
      },
      "source": [
        "# Modele Seq2Seq i atencja\n",
        "Poniższy notebook jest inspirowany tym tutorialem PyTorcha: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html. Zachęcam żeby zajrzeć tam po więcej informacji.\n",
        "\n",
        "W tym notebooku będziemy próbować rozwiązać problem automatycznego tłumaczenia zdań z jednego języka naturalnego na drugi -- konkretniej z języka polskiego na angielski. Dla przykładu model otrzymujący zdanie:\n",
        "\n",
        "> Myślę, że mnie okłamałeś\n",
        "\n",
        "Powinien zwrócić zdanie\n",
        "> I think you lied to me.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do7k4lf0zsxu"
      },
      "source": [
        "# Importy i przygotowanie danych\n",
        "Poniżej znajdują się importy bibliotek potrzebnych do rozwiązania problemu a także skrypt do ładowania zbioru danych zawierającego pary zdań w języku polskim i angielskim. Poniższy kod można odpalić i schować, ale zachęcamy do zaznajomienia się z tym jak wygląda obróbka danych.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCAgsgj3d7Kf"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1yFSn0ld7Ki"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUVJHRTXKoWh",
        "outputId": "2deaa927-342b-471a-c312-e88b70f3d851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-16 22:47:28--  https://raw.githubusercontent.com/gmum/ml2022-23/master/lab/resources/eng-pol.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5081049 (4.8M) [text/plain]\n",
            "Saving to: ‘eng-pol.txt’\n",
            "\n",
            "\reng-pol.txt           0%[                    ]       0  --.-KB/s               \reng-pol.txt         100%[===================>]   4.84M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-01-16 22:47:28 (61.9 MB/s) - ‘eng-pol.txt’ saved [5081049/5081049]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/gmum/ml2022-23/master/lab/resources/eng-pol.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KnEaLlNpire"
      },
      "source": [
        "Poniżej przygotowujemy klasę `Lang` która będzie służyła jako struktura do obsługiwania naszego języka (osobna dla angielskiego i polskiego w naszym przypadku). Do każdego słowa w języku przypisujemy indeks (liczbę porządkową identyfikującą słowo). Dodatkowo definiujemy trzy dodatkowe indeksy:\n",
        "\n",
        "* 0 dla początku zdania (Start of Sentence, SOS)\n",
        "* 1 dla końca zdania (End of Sentence, EOS)\n",
        "* 2 dla paddingu (\"pustych\" wartości). Wartościami tymi będziemy wypełniać zdania w batchu tak, żeby wszystkie były równej długości -- dzięki temu łatwiej będzie zrównoleglić przetwarzanie ich na GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3DtzxXTd7Kk"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "PAD_token = 2\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
        "        self.n_words = 3 # Count SOS, EOS and PAD\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrV27D5Xryh1"
      },
      "source": [
        "Funkcje do normalizowania wchodzących zdań - zamieniamy Unicode na ASCII, zamieniamy wszystkie wielkie litery na małe itd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLLuOE80d7Kk"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = s.replace(\"ł\", \"l\")\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('eng-pol.txt', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    lines = lines[1:]  # Skip first line with attributions.\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')[1::2]] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ld3aJ9Nd7Kl"
      },
      "source": [
        "Wyrzućmy zdania które są zbyt długie (ponad 20 słów)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6fbLbiQd7Km"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 20\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) <= MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) <= MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csmhl8Iod7Km"
      },
      "source": [
        "Pełny proces przetwarzania danych wygląda następująco:\n",
        "\n",
        "- Wczytujemy plik z danymi, dzielimy go na pary zdań.\n",
        "- Normalizujemy tekst\n",
        "- Zamieniamy zdania w listy słów.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRd-fNMAd7Km",
        "outputId": "bfd9a884-a44b-4cb7-e44a-1ac6530c5794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "['sprobujmy cos .', 'let s try something .']\n",
            "Read 59749 sentence pairs\n",
            "Trimmed to 59404 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 29246\n",
            "pol 11953\n",
            "Przykładowe pary zdań:\n",
            "['czy mozemy zmienic temat ?', 'is it ok if we change the subject ?']\n",
            "['nic nie zrobilismy .', 'we didn t do a thing .']\n",
            "['nie ma dla mnie znaczenia czy ona mieszka w miescie czy na wsi .', 'it doesn t matter to me whether she lives in the city or in the countryside .']\n"
          ]
        }
      ],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(pairs[0])\n",
        "    print(f\"Read {len(pairs)} sentence pairs\")\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('pol', 'eng', True)\n",
        "print(\"Przykładowe pary zdań:\")\n",
        "for _ in range(3):\n",
        "    print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQc1t2ArshOa"
      },
      "source": [
        "Na koniec definiujemy jeszcze funkcje, które pozwolą nam zamienić zdania w tensory, które nasza sieć będzie w stanie zrozumieć."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leinIgjGd7Kq"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "\n",
        "def pad_sequences(data_batch):\n",
        "    pl_batch, en_batch = [], []\n",
        "    for pl_sentence, en_sentence in data_batch:\n",
        "        pl_batch += [pl_sentence]\n",
        "        en_batch += [en_sentence]\n",
        "    pl_batch = pad_sequence(pl_batch, padding_value=PAD_token, batch_first=True)\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_token, batch_first=True)\n",
        "    return pl_batch, en_batch\n",
        "\n",
        "def prepare_dataset(batch_size):\n",
        "    rng = np.random.RandomState(567)\n",
        "    indices = np.arange(len(pairs))\n",
        "    rng.shuffle(indices)\n",
        "    train_indices = indices[:int(len(pairs) * 0.8)]\n",
        "    test_indices = indices[int(len(pairs) * 0.8):]\n",
        "    train_pairs = list(pairs[idx] for idx in train_indices)\n",
        "    test_pairs = list(pairs[idx] for idx in test_indices)\n",
        "    tensor_train_pairs = [tensorsFromPair(pairs[idx]) for idx in train_indices]\n",
        "    tensor_test_pairs = [tensorsFromPair(pairs[idx]) for idx in test_indices]\n",
        "    reference_translation = test_pairs\n",
        "\n",
        "    # Output in natural language?\n",
        "\n",
        "    train_loader = DataLoader(tensor_train_pairs, batch_size=batch_size,\n",
        "                            shuffle=True, collate_fn=pad_sequences)\n",
        "    test_loader = DataLoader(tensor_test_pairs, batch_size=batch_size,\n",
        "                            shuffle=True, collate_fn=pad_sequences)\n",
        "    return train_pairs, test_pairs, train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf844auxd7Km"
      },
      "source": [
        "# Model Seq2Seq\n",
        "\n",
        "\n",
        "\n",
        "W tym celu wykorzystany rekurencyjne sieci neuronowe (RNN-y), które poznaliśmy na poprzednich zajęciach. Konkretniej zbudujemy za ich pomocą model Sequence to Sequence (Seq2Seq), w której wykorzystamy dwie sieci rekurencyjne:\n",
        "1. Enkoder, który będzie przyjmował kolejno słowa ze zdania wejściowego i kompresował informacje o nich w swoim stanie ukrytym.\n",
        "2. Dekoder, który będzie generował kolejne słowa w języku docelowym. \n",
        "\n",
        "![seq2seq](https://docs.chainer.org/en/stable/_images/seq2seq.png)\n",
        "Źródło: https://docs.chainer.org/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxaO-7_LtPUk"
      },
      "source": [
        "## Funkcje pomocnicze i ewaluacyjne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Zm6_MRd7Kq"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "\n",
        "def predict(encoder, decoder, inputs, targets=None, max_len=MAX_LENGTH):\n",
        "    batch_size = inputs.size(0)\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(inputs)\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]] * batch_size, device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_output, decoder_attention = decoder(\n",
        "        decoder_input,\n",
        "        decoder_hidden,\n",
        "        targets=targets,\n",
        "        max_len=max_len,\n",
        "        encoder_outputs=encoder_outputs)\n",
        "    return decoder_output, decoder_attention\n",
        "\n",
        "def translate(encoder, decoder, sentence, show_attention=True):\n",
        "    inputs = tensorFromSentence(input_lang, sentence).unsqueeze(0).cuda()\n",
        "    decoder_output, decoder_attention = predict(encoder, decoder, inputs)\n",
        "\n",
        "    decoded_words = []\n",
        "    for word in decoder_output[0]:\n",
        "        top_word = word.argmax(-1).item()\n",
        "        decoded_words.append(output_lang.index2word[top_word])\n",
        "        if top_word == EOS_token:\n",
        "            break\n",
        "\n",
        "    if decoder_attention is not None and show_attention:\n",
        "        # [out_words, in_words]\n",
        "        att = decoder_attention.cpu().detach().numpy()\n",
        "        att = att[0, :len(decoded_words), :]\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        im = ax.imshow(att, vmin=0, vmax=1)\n",
        "        ax.xaxis.tick_top()\n",
        "        ax.set_xticklabels([''] + sentence.split(' ') +\n",
        "                        ['EOS'], rotation=90)\n",
        "        ax.set_yticklabels([''] + decoded_words)\n",
        "        divider = make_axes_locatable(ax)\n",
        "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "\n",
        "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "        \n",
        "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    return decoded_words\n",
        "\n",
        "def batch_translate(encoder, decoder, batch):\n",
        "    decoder_output, decoder_attention = predict(encoder, decoder, batch)\n",
        "\n",
        "    predicted_sentences = []\n",
        "\n",
        "    # TODO: potentially paralellize?\n",
        "    for batch_idx in range(len(batch)):\n",
        "        predicted_words = []\n",
        "        for word in decoder_output[batch_idx]:\n",
        "            top_word = word.argmax(-1).item()\n",
        "            if top_word == EOS_token:\n",
        "                break\n",
        "            predicted_words.append(output_lang.index2word[top_word])\n",
        "\n",
        "        predicted_sentences.append(predicted_words)\n",
        "\n",
        "    return predicted_sentences\n",
        "\n",
        "def dataset_translate(encoder, decoder, loader):\n",
        "    predicted_sentences = []\n",
        "    reference_sentences = [] \n",
        "    for batch_in, batch_out in loader:\n",
        "        translated = batch_translate(encoder, decoder, batch_in)\n",
        "        predicted_sentences.extend(translated)\n",
        "\n",
        "        # TODO: move to a separate file?\n",
        "        reference_words = []\n",
        "        for sentence_idx, sentence in enumerate(batch_out):\n",
        "            decoded_sentence = []\n",
        "            for word in sentence:\n",
        "                if word.item() == EOS_token:\n",
        "                    break\n",
        "                decoded_sentence.append(output_lang.index2word[word.item()])\n",
        "            reference_sentences.append(decoded_sentence)\n",
        "    \n",
        "    return predicted_sentences, reference_sentences\n",
        "\n",
        "\n",
        "def translate_randomly(encoder, decoder, pairs, n=10):\n",
        "    # TODO: reuse translate_given_pairs\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = translate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "\n",
        "def translate_given_pairs(encoder, decoder, pairs):\n",
        "    for pair in pairs:\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = translate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "        \n",
        "def plot_results(bleus, losses):\n",
        "    sns.set_style('whitegrid')\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
        "\n",
        "    axes[0].plot(np.arange(len(bleus)), bleus)\n",
        "    axes[0].set_xlabel(\"Epoka\")\n",
        "    axes[0].set_ylabel(\"BLEU\")\n",
        "    axes[1].plot(np.arange(len(losses)), losses)\n",
        "    axes[1].set_xlabel(\"Epoka\")\n",
        "    axes[1].set_ylabel(\"Koszt na zbiorze treningowym\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKEO71h8_mvq"
      },
      "source": [
        "## Pętla trenująca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2hJ24pEd7Kq"
      },
      "outputs": [],
      "source": [
        "def train(encoder, decoder, lr=0.01, batch_size=256, teacher_forcing_ratio=0.5, epochs_num=100, clipping=1.0):\n",
        "\n",
        "    # Prepare dataset, loss functions, optimizer\n",
        "    train_pairs, test_pairs, train_loader, test_loader = prepare_dataset(batch_size)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
        "\n",
        "    encoder.cuda()\n",
        "    decoder.cuda()\n",
        "\n",
        "    bleus = []\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(epochs_num + 1):\n",
        "\n",
        "        # Training\n",
        "        epoch_train_loss = 0.\n",
        "        for in_batch, out_batch in train_loader:\n",
        "            in_batch, out_batch = in_batch.cuda(), out_batch.cuda()\n",
        "\n",
        "            encoder_optimizer.zero_grad()\n",
        "            decoder_optimizer.zero_grad()\n",
        "        \n",
        "            teacher_inputs = out_batch if random.random() < teacher_forcing_ratio else None\n",
        "        \n",
        "            decoder_output, decoded_attention = predict(\n",
        "                encoder, decoder, in_batch,\n",
        "                targets=teacher_inputs,\n",
        "                max_len=out_batch.size(1)\n",
        "            )\n",
        "\n",
        "            loss = criterion(decoder_output.transpose(1, 2), out_batch)\n",
        "            \n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            nn.utils.clip_grad_norm_(encoder.parameters(), clipping)\n",
        "            nn.utils.clip_grad_norm_(decoder.parameters(), clipping)\n",
        "\n",
        "            encoder_optimizer.step()\n",
        "            decoder_optimizer.step()\n",
        "\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "        # Evaluation\n",
        "        if epoch % 1 == 0:\n",
        "            with torch.no_grad():\n",
        "                print(\"=\" * 25, \"Translation test\", \"=\" * 25)\n",
        "                translate_randomly(encoder, decoder, test_pairs, n=5)\n",
        "\n",
        "            pred_sentences, ref_sentences = dataset_translate(encoder, decoder, test_loader)\n",
        "            bleu_val = bleu_score(pred_sentences, [[sentence] for sentence in ref_sentences])\n",
        "            print(\"=\" * 25, f\"BLEU: {bleu_val}\", \"=\" * 25)\n",
        "            bleus += [bleu_val]\n",
        "\n",
        "        mean_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses += [mean_train_loss]\n",
        "        print(f\"Epoch: {epoch}. Train loss: {mean_train_loss}\")\n",
        "    return bleus, train_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkvoS5hCjgI2"
      },
      "source": [
        "# Zadanie 1 - Dekoder w Seq2Seq (4 pkt.)\n",
        "\n",
        "W tym zadaniu należy zaimplementować dekoder z modelu Seq2Seq. Kod enkodera jest dostępny poniżej i ma Państwu ułatwić odpowiednie zaimplementowanie dekodera.\n",
        "\n",
        "\n",
        "\n",
        "Dekoder otrzymuje na wejściu następujące argumenty:\n",
        "- `input` - tensor o wymiarach `[batch_size, 1]` zawierający słowo `<BOS>`. Powinno być podane w pierwszym kroku wykonywania dekodera.\n",
        "- `hidden` - ostatnia reprezentacja ukryta z enkodera .\n",
        "- `targets` - `None` albo `torch.tensor` o wymiarach `[batch_size, seq_len]` zawierający indeksy słów w języku docelowym. Jeżeli jest podany to należy zaimplementować teacher forcing na jego podstawie.\n",
        "- `max_len` - Długość sekwencji, którą mamy zwrócić.\n",
        "- `encoder_outputs` - w tym zadaniu ten argument należy zignorować, przyda się dopiero w kolejnym zadaniu.\n",
        "\n",
        "Dekoder ma zwrócić dwie zmienne:\n",
        "- `output` - tensor o wymiarach `[batch_size, max_len, vocab_size]` reprezentujące logity, które po zaaplikowaniu softmaksa (co będzie zrobione już poza dekoderem) będą reprezentowały prawdopodobieństwa słów przewidzianych przez nasz dekoder.\n",
        "- `attention_weights` - w tym zadaniu należy zawsze zwracać `None`.\n",
        "\n",
        "Architektura głowy klasyfikacyjnej jest dowolna, natomiast zalecamy sieć z jedną warstwą ukrytą: `[hidden_size, hidden_size, vocab_size]` i aktywacją tanh.\n",
        "\n",
        "\n",
        "**HINT 1**: Warto pamiętać o argumencie `batch_first=True` przy definiowaniu RNN-a.\n",
        "\n",
        "**HINT 2**: W enkoderze mogliśmy użyć jednego wywołania klasy GRU, jako że od razu mieliśmy wszystkie wejścia (słowa języka wejściowego). W przypadku dekodera nie jest to możliwe, jako że wejściem w kroku `t+1` jest wyjście z kroku `t`. Oznacza to że prawdopodobnie potrzebna będzie pętla `for`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6FSVbB0d7Kn"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_cell = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.rnn_cell(embedded)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiBbM1IVd7Ko"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size,hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, targets=None, max_len=None, encoder_outputs=None):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        #print(\"tutaj\")\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "4KiePQdpivCd",
        "outputId": "d0954f4d-6faf-4e9b-8fb6-6cc1023b2c80",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-9884b1f1a3f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbleus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbleus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-26b0b0b0c57b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, lr, batch_size, teacher_forcing_ratio, epochs_num, clipping)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mteacher_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_batch\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             decoder_output, decoded_attention = predict(\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-3870b15b40c4>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(encoder, decoder, inputs, targets, max_len)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSOS_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     decoder_output, decoder_attention = decoder(\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-0beed592c3f1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, targets, max_len, encoder_outputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tutaj\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    208\u001b[0m                     expected_input_dim, input.dim()))\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    211\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[1;32m    212\u001b[0m                     self.input_size, input.size(-1)))\n",
            "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 512, got 262144"
          ]
        }
      ],
      "source": [
        "hidden_size = 1024\n",
        "embedding_size = 512\n",
        "lr = 1e-3\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, embedding_size, hidden_size).to(device)\n",
        "decoder = DecoderRNN(output_lang.n_words, embedding_size, hidden_size).to(device)\n",
        "\n",
        "bleus, losses = train(encoder, decoder, batch_size=512, lr=lr, epochs_num=10, clipping=0.1)\n",
        "plot_results(bleus, losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crrwsvmgGHix"
      },
      "source": [
        "# Zadanie 2 - Atencja w RNN-ach (3 pkt.)\n",
        "\n",
        "![seq2seq z dekoderem](https://www.researchgate.net/profile/Chandan_Reddy6/publication/329464533/figure/fig3/AS:701043021197314@1544153089772/An-attention-based-seq2seq-model.ppm)\n",
        "\n",
        "Źródło: https://github.com/google/seq2seq\n",
        "\n",
        "W tym zadaniu należy napisać kod nowego dekodera, który ma działać podobnie jak dekoder w poprzednim zadaniu, ale jednocześnie ma wykorzystywać mechanizm atencji.\n",
        "\n",
        "W normalnym dekoderze, w kroku `t` wejściem do komórki GRU (pomijamy tutaj przekazywanie stanu ukrytego) była wyłącznie zembeddowana reprezentacja $\\bar{y}_t$. W dekoderze z atencją na wejściu podawna będzie konkatenacja tego wektora oraz specjalnego wektora $z_t$ stworzonego na podstawie wyjść z enkodera: $\\tilde{h}_t = [\\bar{y}_t, z_t]$. \n",
        "\n",
        "Wektor $z_t$ jest pozyskiwany za pomocą mechanizmu atencji. Intuicyjnie chcielibyśmy w nim zebrać informacje z enkodera, które będą najistotniejsze przy dekodowaniu aktualnego słowa. Przyjmijmy, że mamy funkcję alignmentu $a(h, e)$, która jest nam w stanie powiedzieć jak bardzo podobne do siebie są stan ukryty dekodera $h$ oraz reprezentacja słowa $e$.\n",
        "\n",
        "Wtedy \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "w_i &= \\frac{ \\exp(a(h, e_i)) }{\\sum_{j} \\exp(a(h, e_j))} \\\\\n",
        "z_t &= \\sum_i e_i \\cdot w_i\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "W naszym wypadku funkcja alignmentu $a(h, e)$ ma być siecią neuronową z dwoma warstwami o kolejnych wymiarach: `[2 * hidden_size, hidden_size, 1]` i aktywacją tanh po pierwszej warstwie.\n",
        "\n",
        "Argumenty wejściowe i wyjściowe z dekodera są takie same jak w poprzednim z zadaniu z wyjątkiem:\n",
        "- Tym razem na wejściu otrzymujemy tensor `encoder_outputs` o wymiarach `[batch_size, encoder_seq_len, hidden_size]`. To są reprezentacje $e_i$, które należy wykorzystać w mechanizmie atencji.\n",
        "- Tym razem na wyjściu `attention_weights` powinno być tensorem o wymiarach `[batch_size, decoder_seq_len, encoder_seq_len]` zawierającym wagi $w_i$. **HINT:** wartości tego tensora powinny się sumować do jedynki na ostatnim wymiarze."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bETuU9Hd7Kp"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, targets=None, max_len=None, encoder_outputs=None):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, seq_att_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "mipeLXX9fx_7",
        "outputId": "20714792-a8da-4bdf-8197-cf2de23f5d32",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-a198dea2a363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbleus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-87006db84041>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, embedding_size, hidden_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'output_size' is not defined"
          ]
        }
      ],
      "source": [
        "hidden_size = 1024\n",
        "embedding_size = 512\n",
        "lr = 1e-3\n",
        "clip = 0.1\n",
        "encoder = EncoderRNN(input_lang.n_words, embedding_size, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(output_lang.n_words, embedding_size, hidden_size).to(device)\n",
        "\n",
        "bleus, losses = train(encoder, decoder, lr=lr, batch_size=512, epochs_num=10, clipping=0.1)\n",
        "plot_results(bleus, losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPmEeponfgob"
      },
      "source": [
        "# Powiązana literatura\n",
        "\n",
        "* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
        "* [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
        "\n",
        "## Przydatne tutoriale\n",
        "\n",
        "* https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "* https://github.com/bentrevett/pytorch-seq2seq\n",
        "* https://github.com/gmum/AppliedDL2020/tree/master/Week%207 - materiały z kursu Applied Deep Learning prowadzonego w semestrze letnim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAoJGLVhV_-y"
      },
      "source": [
        "# Zadanie dodatkowe: Transformer (7 pkt.)\n",
        "\n",
        "Na podstawie pracy [Attention is All You Need](https://arxiv.org/abs/1706.03762) oraz strony [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention) zaimplementuj transformera działającego na powyższym zadaniu tłumaczenia z polskiego na angielski.\n",
        "\n",
        "![transformer](http://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}